{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6999293,"sourceType":"datasetVersion","datasetId":4023569},{"sourceId":7091447,"sourceType":"datasetVersion","datasetId":4062379}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-30T16:25:36.968577Z","iopub.execute_input":"2023-11-30T16:25:36.969060Z","iopub.status.idle":"2023-11-30T16:25:36.982070Z","shell.execute_reply.started":"2023-11-30T16:25:36.969025Z","shell.execute_reply":"2023-11-30T16:25:36.980802Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"/kaggle/input/hatespeech/combined_file.csv\n/kaggle/input/hatespeech/train_data_70.csv\n/kaggle/input/hatespeech/train_80_data.csv\n/kaggle/input/hatespeech/unseen_test.csv\n/kaggle/input/hatespeech/train-final.csv\n/kaggle/input/hatespeech/train_data_75.csv\n/kaggle/input/hatespeech/test-final.csv\n/kaggle/input/hatespeech/test_20_data.csv\n/kaggle/input/hatespeech/test_data_30.csv\n/kaggle/input/hatespeech/test_data_25.csv\n/kaggle/input/training/train.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\nfrom scipy.sparse import hstack\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport re\nfrom textblob import TextBlob\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nimport string\nfrom textblob import TextBlob","metadata":{"execution":{"iopub.status.busy":"2023-11-30T16:25:36.984609Z","iopub.execute_input":"2023-11-30T16:25:36.985091Z","iopub.status.idle":"2023-11-30T16:25:36.996047Z","shell.execute_reply.started":"2023-11-30T16:25:36.985049Z","shell.execute_reply":"2023-11-30T16:25:36.994630Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# Download NLTK resources (if not downloaded)\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')","metadata":{"execution":{"iopub.status.busy":"2023-11-30T16:25:36.998235Z","iopub.execute_input":"2023-11-30T16:25:36.998604Z","iopub.status.idle":"2023-11-30T16:25:37.014522Z","shell.execute_reply.started":"2023-11-30T16:25:36.998573Z","shell.execute_reply":"2023-11-30T16:25:37.013217Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"# Load training and testing datasets\ntrain_data = pd.read_csv('/kaggle/input/hatespeech/train_data_70.csv')  # Replace 'training_data.csv' with your training dataset file\ntest_data = pd.read_csv('/kaggle/input/hatespeech/test_data_30.csv')    # Replace 'testing_data.csv' with your testing dataset file\n","metadata":{"execution":{"iopub.status.busy":"2023-11-30T16:25:37.016250Z","iopub.execute_input":"2023-11-30T16:25:37.016634Z","iopub.status.idle":"2023-11-30T16:25:37.090172Z","shell.execute_reply.started":"2023-11-30T16:25:37.016600Z","shell.execute_reply":"2023-11-30T16:25:37.089190Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# Preprocessing text data\nlemmatizer = WordNetLemmatizer()\nstop_words = set(stopwords.words('english'))","metadata":{"execution":{"iopub.status.busy":"2023-11-30T16:25:37.092910Z","iopub.execute_input":"2023-11-30T16:25:37.093444Z","iopub.status.idle":"2023-11-30T16:25:37.100016Z","shell.execute_reply.started":"2023-11-30T16:25:37.093399Z","shell.execute_reply":"2023-11-30T16:25:37.099191Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"\ndef preprocess_text(text):\n    text = re.sub(r'[^a-zA-Z]', ' ', text)  # Remove non-alphabetic characters\n    tokens = word_tokenize(text.lower())  # Tokenization and lowercase\n    tokens = [word for word in tokens if word not in stop_words]  # Removing stopwords without lemmatization\n    return ' '.join(tokens)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-30T16:25:37.101370Z","iopub.execute_input":"2023-11-30T16:25:37.101902Z","iopub.status.idle":"2023-11-30T16:25:37.114082Z","shell.execute_reply.started":"2023-11-30T16:25:37.101870Z","shell.execute_reply":"2023-11-30T16:25:37.112766Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# # Extended text preprocessing function\n# def preprocess_text_extended(text):\n#     # Remove punctuation and symbols\n#     text = re.sub(r'[^\\w\\s]', '', text)\n    \n#     # Tokenization\n#     tokens = word_tokenize(text)\n    \n#     # Remove stopwords and normalize\n#     stop_words = set(stopwords.words('english'))\n#     filtered_tokens = [word.lower() for word in tokens if word.lower() not in stop_words]\n    \n#     # Sentiment analysis\n#     sentiment = TextBlob(text).sentiment.polarity\n    \n#     # Stemming\n#     stemmer = PorterStemmer()\n#     stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n    \n#     return ' '.join(stemmed_tokens)\n\n# train_data['processed_text'] = train_data['text'].apply(preprocess_text_extended)\n# train_data['processed_context'] = train_data['context'].apply(preprocess_text_extended)\n\n# test_data['processed_text'] = test_data['text'].apply(preprocess_text_extended)\n# test_data['processed_context'] = test_data['context'].apply(preprocess_text_extended)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T16:25:37.115502Z","iopub.execute_input":"2023-11-30T16:25:37.116020Z","iopub.status.idle":"2023-11-30T16:25:37.125545Z","shell.execute_reply.started":"2023-11-30T16:25:37.115989Z","shell.execute_reply":"2023-11-30T16:25:37.124711Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# Extended text preprocessing function\ndef preprocess_text_extended(text):\n    # Remove punctuation and symbols\n    text = re.sub(r'[^\\w\\s]', '', text)\n    \n    # Tokenization\n    tokens = word_tokenize(text)\n    \n    # Remove stopwords and normalize\n    stop_words = set(stopwords.words('english'))\n    filtered_tokens = [word.lower() for word in tokens if word.lower() not in stop_words]\n    \n    # Sentiment analysis\n    sentiment = TextBlob(text).sentiment.polarity\n    \n    # Stemming\n    stemmer = PorterStemmer()\n    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n    \n    return ' '.join(stemmed_tokens)\n\ntrain_data['processed_text'] = train_data['text'].apply(preprocess_text_extended)\ntrain_data['processed_context'] = train_data['context'].apply(preprocess_text_extended)\n\ntest_data['processed_text'] = test_data['text'].apply(preprocess_text_extended)\ntest_data['processed_context'] = test_data['context'].apply(preprocess_text_extended)\n\n# Feature extraction using TF-IDF separately for text and context in training data\ntfidf_vectorizer_text = TfidfVectorizer(max_features=1000)  # You can adjust max_features as needed\nX_train_text = tfidf_vectorizer_text.fit_transform(train_data['processed_text'].astype(str))\n\ntfidf_vectorizer_context = TfidfVectorizer(max_features=1000)  # You can adjust max_features as needed\nX_train_context = tfidf_vectorizer_context.fit_transform(train_data['processed_context'].astype(str))\n\n# Feature extraction using TF-IDF separately for text and context in testing data\nX_test_text = tfidf_vectorizer_text.transform(test_data['processed_text'].astype(str))\nX_test_context = tfidf_vectorizer_context.transform(test_data['processed_context'].astype(str))\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-30T16:25:37.177400Z","iopub.execute_input":"2023-11-30T16:25:37.178176Z","iopub.status.idle":"2023-11-30T16:25:55.564937Z","shell.execute_reply.started":"2023-11-30T16:25:37.178109Z","shell.execute_reply":"2023-11-30T16:25:55.563620Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# Combine the feature matrices for training and testing\n# X_train_combined = hstack((X_train_text, X_train_context))\n# X_test_combined = hstack((X_test_text, X_test_context))\nX_train_combined= X_train_text\nX_test_combined = X_test_text\ny_train = train_data['label']\ny_test = test_data['label']\n\n# Training the logistic regression model\nlogreg = LogisticRegression()\nlogreg.fit(X_train_combined, y_train)\n\n# Predictions on the test set\ny_pred = logreg.predict(X_test_combined)\n\n# Model evaluation\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Additional evaluation metrics (optional)\nprint(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-11-30T16:25:55.567056Z","iopub.execute_input":"2023-11-30T16:25:55.567440Z","iopub.status.idle":"2023-11-30T16:25:55.668640Z","shell.execute_reply.started":"2023-11-30T16:25:55.567407Z","shell.execute_reply":"2023-11-30T16:25:55.667220Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Accuracy: 0.70\n              precision    recall  f1-score   support\n\n           0       0.71      0.88      0.79      1803\n           1       0.65      0.38      0.48      1047\n\n    accuracy                           0.70      2850\n   macro avg       0.68      0.63      0.63      2850\nweighted avg       0.69      0.70      0.67      2850\n\n","output_type":"stream"}]},{"cell_type":"code","source":"\n\n# Calculate Accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Calculate Precision\nprecision = precision_score(y_test, y_pred, average='weighted')\nprint(f\"Precision: {precision:.2f}\")\n\n# Calculate Recall\nrecall = recall_score(y_test, y_pred, average='weighted')\nprint(f\"Recall: {recall:.2f}\")\n\n# Calculate F1 Score\nf1 = f1_score(y_test, y_pred, average='weighted')\nprint(f\"F1 Score: {f1:.2f}\")\n\n# Classification Report (Includes Precision, Recall, F1-score)\nprint(classification_report(y_test, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2023-11-30T16:25:55.670295Z","iopub.execute_input":"2023-11-30T16:25:55.670705Z","iopub.status.idle":"2023-11-30T16:25:55.703642Z","shell.execute_reply.started":"2023-11-30T16:25:55.670670Z","shell.execute_reply":"2023-11-30T16:25:55.702337Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"Accuracy: 0.70\nPrecision: 0.69\nRecall: 0.70\nF1 Score: 0.67\n              precision    recall  f1-score   support\n\n           0       0.71      0.88      0.79      1803\n           1       0.65      0.38      0.48      1047\n\n    accuracy                           0.70      2850\n   macro avg       0.68      0.63      0.63      2850\nweighted avg       0.69      0.70      0.67      2850\n\n","output_type":"stream"}]},{"cell_type":"code","source":"name = 'LR 70-30'\ntest_data[name] = y_pred\ntest_data.to_csv(name+'.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T16:25:55.705221Z","iopub.execute_input":"2023-11-30T16:25:55.705579Z","iopub.status.idle":"2023-11-30T16:25:55.759881Z","shell.execute_reply.started":"2023-11-30T16:25:55.705541Z","shell.execute_reply":"2023-11-30T16:25:55.758691Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"##### ","metadata":{}}]}